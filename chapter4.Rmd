# Clustering and classification (Exercise 4)

```{r}
library(MASS);library(tidyverse);library(corrplot)
data("Boston")

```
The data has 506 observations and 14 variables. Data is about housing values in suburbs of Boston. 

### Explort dataset
```{r}
str(Boston)
summary(Boston)
pairs(Boston)
```

### Correlate all variables
```{r}
cor_matrix<-cor(Boston)
cor_matrix%>%round(2)
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex=0.6)
```
There is strongly positive correlation bewtween "rad" and "tax" and stongly negative correlation between "age" and "dis" and between "lstat" and "medv". 

### Standardized data
```{r}
boston_scaled<-scale(Boston)
summary(boston_scaled)

```
### Covert matrix data to dafaframe
```{r}
class(boston_scaled)
boston_scaled<-as.data.frame(boston_scaled)

```

### Create a catergorical variable with the variable "Crime" 
```{r}
summary("crim")
bins<-quantile(boston_scaled$crim)
bins
crime<-cut(boston_scaled$crim, breaks=bins, include.lowest=TRUE, label=c("low","med_low","med_high", "high"))
crime
table(crime)
```

### Remove original "crim" from the dataset and add the new categorial variable
```{r}
boston_scaled<-dplyr::select(boston_scaled,-crim)
boston_scaled
boston_scaled<-data.frame(boston_scaled, crime)

```

### Divide the dataset into train and test
```{r}
n<-nrow(boston_scaled)
ind<-sample(n, size = n*0.8)
train<-boston_scaled[ind, ]
test<-boston_scaled[-ind, ]
correct_classes<-test$crime
```

## Linear Discriminant Analysis
```{r}
lda.fit<-lda(crime~., data=train)
lda.fit
lda.arrows<-function(x, myscale=1, arrow_heads= 0.1, color="red", tex=0.75, choices =c (1,2)){
  heads<-coef(x)
  arrows(x0 = 0, y0=0, x1 = myscale*heads[,choices[1]],y1=myscale*heads[,choices[2]], col=color, length= arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
```

### Convert target classes to numeric data
```{r}
classes<-as.numeric(train$crime)
```

### Plot the lda results
```{r}
plot(lda.fit, dimen=2, col=classes, pch=classes)
lda.arrows(lda.fit, myscale=1)
```

## Predict the classes with the LDA model and Cross-tabulate the results
```{r}
lda.pred<-predict(lda.fit, newdata = test)
table(correct=correct_classes, predict=lda.pred$class)
```

#### Distance measures
```{r}
library(MASS)
data("Boston")
boston_scaled<-scale(Boston)
boston_scaled<-as.data.frame(boston_scaled)
```
##### Euclidean distane matrix
```{r}
dist_eu<-dist(boston_scaled)
summary(dist_eu)
```

#### K-means
```{r}
km<-kmeans(boston_scaled, centers=3)
pairs(boston_scaled, col=km$cluster)
```

#### Investigate the optimal number of clusters
```{r}
set.seed(11)
k_max<-20
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})
qqplot(x=1:k_max, y = twcss, geom="Line")
```

#### Perform k-means with different number of clusters
```{r}
km<-kmeans(boston_scaled, centers=6)
pairs(boston_scaled, col = km$cluster)
library(GGally)
ggpairs(boston_scaled)
```

#### Super Bonus
```{r}
model_predictors<- dplyr::select(train, -crime)
```
##### Check th dimensions
```{r}
dim(model_predictors)
dim(lda.fit$scaling)
```
##### Matrix multiplication
```{r}
matrix_product <- as.matrix(model_predictors)%*%lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)
```
##### Create a 3D plot
```{r}
library(plotly)
plot_ly(x=matrix_product$LD1, y=matrix_product$LD2, z=matrix_product$LD3, type="scatter3d", mode="markers")
```
```{r}
plot_ly(x=matrix_product$LD1, y=matrix_product$LD2, z=matrix_product$LD3, type="scatter3d", mode="markers", color = train$crime)
```
```{r}
km <-kmeans(boston_scaled, centers = 6)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = km$cluster[ind])
```

